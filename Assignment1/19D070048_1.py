# -*- coding: utf-8 -*-
"""19D070048_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_2Dneu93t9VgdNWUfADK8FDGKF4vVXD8

#**EE769 Introduction to Machine Learning**

#Assignment 1: Gradient Descent, Linear Regression, and Regularization


**Template and Instructions**



1. Up to two people can team up, but only one should submit, and both should understand the entire code.
2. Every line of code should end in a comment explaining the line
3. It is recommended to solve the assignment in Google Colab.
Write your roll no.s separated by commas here: 19D070048
4. Write your names here:Puranjay Datta 
5. There are two parts to the assignment. In the Part 1, the code format has to be strictly followed to enable auto-grading. In the second part, you can be creative.
6. **You can discuss with other groups or refer to the internet without being penalized, but you cannot copy their code and modify it. Write every line of code and comment on your own.**

#**Part 1 begins ...**
**Instructions to be strictly followed:**

1. Do not add any code cells or markdown cells until the end of this part. Especially, do not change the blocks that say "TEST CASES, DO NOT CHANGE"
2. In all other cells only add code where it says "CODE HERE".
3. If you encounter any raise NotImplementedError() calls you may comment them out.

We cannot ensure correct grading if you change anything else, and you may be penalised for not following these instructions.

## Import Statements
"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

"""## Normalize function 


"""

def Normalize(X): # Output should be a normalized data matrix of the same dimension
    Y=np.zeros(np.shape(X))
    if X.ndim==1 :
      Y = (X-np.mean(X))/np.std(X)  #Handling 1D array
    else :
      
      for col in range(0,len(X[0])):
        Y[:,col] = (X[:,col]-np.mean(X[:,col]))/np.std(X[:,col])  # normalized matrix
        
    return Y
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 - 1 dimensional array'''
#X=np.array([[1,2,3],[3,4,5],[7,8,9]])
X1=np.array([1,2,3])
np.testing.assert_array_almost_equal(Normalize(X1),np.array([-1.224,  0.      ,  1.224]),decimal=3)
''' case 2 - 2 dimensional array'''
X2=np.array([[4,7,6],[3,8,9],[5,11,10]])
np.testing.assert_array_almost_equal(Normalize(X2),np.array([[ 0.  , -0.980581, -1.372813],[-1.224745, -0.392232,  0.392232],[ 1.224745,  1.372813,  0.980581]]))
''' case 3 - 1 dimensional array with float'''
X3=np.array([5.5,6.7,3.2,6.7])
np.testing.assert_array_almost_equal(Normalize(X3),np.array([-0.017,  0.822, -1.627,  0.822]),decimal=3)

"""## Prediction Function

Given X and w, compute the predicted output. Do not forget to add 1's in X
"""

def Prediction (X, w): # Output should be a prediction vector y
    r=len(X)
    X=np.c_[ X, np.ones(r)];  #Adding a column of 1's
    return np.dot(X,w)
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 - Known input output matrix and weights 1'''
X1 = np.array([[3,2],[1,1]])
w1 = np.array([2,1,1]) 
np.testing.assert_array_equal(Prediction(X1,w1),np.array([9,4]))

"""## Loss Functions

Code the four  loss functions:

1. MSE loss is only for the error
2. MAE loss is only for the error
3. L2 loss is for MSE and L2 regularization, and can call MSE loss
4. L1 loss is for MSE and L1 regularization, and can call MSE loss
"""

def MSE_Loss (X, t, w, lamda =0): # Ouput should be a single number
    return np.mean(np.square(Prediction(X, w)-t))   #MSE=(x1^2+x2^2+...xn^2)/n
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_almost_equal(MSE_Loss(X,t,w),0.53,decimal=3)

def MAE_Loss (X, t, w, lamda = 0): # Output should be a single number
    return np.mean(np.abs(Prediction(X, w)-t))  #MAE=(|x1|+|x2|+...|xn|)/n
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_almost_equal(MAE_Loss(X,t,w),0.700,decimal=3)

def L2_Loss (X, t, w, lamda): # Output should be a single number based on L2-norm (with sqrt)
    return  MSE_Loss(X,t,w)+lamda*np.linalg.norm(w[0:len(w)-1],ord=2)  #L2_Loss=MSE+ lamda*(w1^2+w2^2+...wn^2)^0.5
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_almost_equal(L2_Loss(X,t,w,0.5),1.675,decimal=3)

def L1_Loss (X, t, w, lamda): # Output should be a single number
    return MSE_Loss(X,t,w)+lamda*np.linalg.norm(w[0:len(w)-1],ord=1) #L1_Loss=MSE+lamda*(|w1|+|w2|+...|wn|)
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_almost_equal(L1_Loss(X,t,w,0.5),2.280,decimal=3)

def NRMSE_Metric (X, t, w, lamda=0): # Output should be a single number. RMSE/std_dev(t)
    return np.sqrt(MSE_Loss(X,t,w))/np.std(t);
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' Test case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_almost_equal(NRMSE_Metric(X,t,w,0.5),0.970,decimal=3)

"""## Gradient function
Each Loss function will have its own gradient function:

1. MSE gradient is only for the error
2. MAE gradient is only for the error
3. L2 gradient is for MSE and L2 regularization, and can call MSE gradient
4. L1 gradient is for MSE and L1 regularization, and can call MSE gradient
"""

def MSE_Gradient (X, t, w, lamda=0): # Output should have the same size as w
    grad=np.zeros(len(w));#initialize 
    r=len(X)  #no. of rows of dataset
    X_new=np.c_[ X, np.ones(r)];  #Adding the bias
    for i in range(0,len(w)):
        grad[i]=(-2/len(X_new))*np.dot((t-Prediction(X,w)),X_new[:,i]) #(-2/n)sum[(ti-wi.xi)xi]
    return grad
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_array_almost_equal(MSE_Gradient(X,t,w),np.array([2.55, 2.94, 2.9 , 0.4 ]),decimal=3)

def MAE_Gradient (X, t, w, lamda=0): # Output should have the same size as w
    grad=np.zeros(len(w));#initialize 
    r=len(X)   #no. of rows of dataset
    X_new=np.c_[ X, np.ones(r)]; #Adding the bias
    for i in range(0,len(w)):
        grad[i]=(-1/len(X_new))*np.dot(np.sign((t-Prediction(X,w))),X_new[:,i])#(-1/n)sum[sign{(ti-wi.xi)}xi]
    return grad
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_array_almost_equal(MAE_Gradient(X,t,w),np.array([0.75,  0.3 ,  0.5 , 0.]),decimal=3)

def L2_Gradient (X, t, w, lamda): # Output should have the same size as w
    grad=np.zeros(len(w)); #initialize
    for i in range(0,len(w)-1):
      grad[i]=(lamda*w[i]/np.linalg.norm(w[0:len(w)-1],ord=2))  #MSE gradient + sum[lamda*wi/Norm_without_bias]
    
    return grad+MSE_Gradient(X,t,w)
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_array_almost_equal(L2_Gradient(X,t,w,0.5),np.array([2.986, 2.721, 3.009 , 0.4 ]),decimal=3)

def L1_Gradient (X, t, w, lamda): # Output should have the same size as w
    grad=np.zeros(len(w));#initialize
    for i in range(0,len(w)-1):
        grad[i]=(lamda*np.sign(w[i]))  #MSE gradient + sum[lamda*sign(wi)]
        
    return grad+MSE_Gradient(X,t,w)
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 '''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
w=np.array([2,-1,0.5,1])
np.testing.assert_array_almost_equal(L1_Gradient(X,t,w,0.5),np.array([3.05, 2.44, 3.4 , 0.4 ]),decimal=3)

"""## Gradient Descent Function

"""

def Gradient_Descent (X, X_val, t, t_val, w, lamda, max_iter, epsilon, lr, lossfunc, gradfunc): # See output format in 'return' statement
    previous_loss = None
    current_loss=None
  
    for i in range(0,max_iter):
             
        # Calculationg the current cost
        current_loss = lossfunc(X,t,w,lamda)
        #Calculating gradient
        grad_w=gradfunc(X,t,w,lamda)
        #Calculating the updated weights
        w=w-lr*grad_w
        if previous_loss and abs(previous_loss-current_loss)<epsilon:  # if difference between current and previous error is less than eps then stop iteration
            break
         
        previous_loss = current_loss
        
    
    train_loss_final=lossfunc(X,t,w,lamda)
    validation_loss_final=lossfunc(X_val,t_val,w,lamda)
    validation_NRMSE=NRMSE_Metric(X_val,t_val,w)
    return w, train_loss_final, validation_loss_final, validation_NRMSE #You should return variables structured like this

'''
TEST CASES, DO NOT CHANGE
'''
X=np.array([[23,24],[1,2]])
t=np.array([4,5])
X_val=np.array([[3,4],[5,6]])
t_val=np.array([3,4])
w=np.array([3,2,1])
results =Gradient_Descent (X, X_val, t, t_val, w, 0.1, 100, 1e-10, 1e-5, L2_Loss,L2_Gradient) 
np.testing.assert_allclose([results[1]],[697.919],rtol =0.05)
np.testing.assert_allclose([results[2]],[20],atol=5) # we expect around 17.5  but some students got 24 which we will also accept
#Instructor Values of results[1] and results [2] are 697.919 and 17.512 respectively

"""## Pseudo Inverse Method

You have to implement a slightly more advanced version, with L2 penalty:

w = (X' X + lambda I)^(-1) X' t.

See, for example: Section 2 of https://web.mit.edu/zoya/www/linearRegression.pdf

Here, the column of 1's in assumed to be included in X
"""

def Pseudo_Inverse (X, t, lamda): # Output should be weight vector
    r=len(X)
    X_new=np.c_[ X, np.ones(r)];
    I=np.identity(len(X_new[0])) #Identity Matrix
    X_t=np.transpose(X_new) #Xtranspose=X'
    w=np.dot(np.dot(np.linalg.inv((np.dot(X_t,X_new)+lamda*I)),X_t),t) #(X' X + lambda I)^(-1) X' t
    return w
    raise NotImplementedError()

'''
TEST CASES, DO NOT CHANGE
'''
''' case 1 - other data'''
X=np.array([[3,6,5],[4.5,6.6,6]])
t=np.array([4,5.5])
np.testing.assert_array_almost_equal(Pseudo_Inverse(X,t,0.5),np.array([ 0.491,  0.183,  0.319, -0.002]),decimal=3)

"""#... Part 1 ends Below this you be more creative. Just comment out the lines where you save files (e.g. test predictions).

#**Part 2 begins ...**

**Instructions to be loosely followed (except number 8):**

1. Add more code and text cells between this and the last cell.
2. Read training data from: https://www.ee.iitb.ac.in/~asethi/Dump/TempTrain.csv only. Do not use a local copy of the dataset.
3. Find the best lamda for **MSE+lamda*L2(w)** loss function. Plot training and validation RMSE vs. 1/lamda (1/lamda represents model complexity). Print weights, validation RMSE, validation NRMSE for the best lamda.
4. Find the best lamda for **MSE+lamda*L1(w)** loss function. Plot training and validation RMSE vs. 1/lamda (1/lamda represents model complexity). Print weights, validation RMSE, validation NRMSE for the best lamda.
5. Find the best lamda for the **pseudo-inv method**. Plot training and validation RMSE vs. 1/lamda (1/lamda represents model complexity). Print weights, validation RMSE, validation NRMSE for the best lamda.
6. Write your observations and conclusions.
7. Read test data from: https://www.ee.iitb.ac.in/~asethi/Dump/TempTest.csv only. Do not use a local copy of the dataset. Predict its dependent (missing last column) using the model with the lowest MSE, RMSE, or NRMSE. Save it as a file RollNo1_RollNo2_1.csv.
8. **Disable the prediction csv file saving statement and submit this entire .ipynb file, .py file, and .csv file as a single RollNo1_RollNo2_1.zip file.**
"""

#Reading the Training dataframe
url="https://www.ee.iitb.ac.in/~asethi/Dump/TempTrain.csv "
df=pd.read_csv(url)

#For splitting training & validation dataset in 80:20 ratio
row=np.int(len(df)*0.8)

#Extracting the features for training 
train_df=df[df.columns.values[0:21]].iloc[:row,:]

#Extracting the target for training dataframe 
t=np.array(df[df.columns.values[21]].iloc[:row])

#Normalizing & adding bias
#X=Normalize(np.asarray(train_df))
X=np.asarray(train_df)
X=np.c_[ X, np.ones(len(X))]

#initialize with array of 1's for gradient descent
w=np.ones(len(X[0])+1)*0.1

#Extracting Validation dataframe
validation_df=df[df.columns.values[0:21]].iloc[row+1:,:]

#Extracting the target for validation dataframe 
t_val=np.array(df[df.columns.values[21]].iloc[row+1:])

#Normalizing & adding bias
X_val=np.asarray(validation_df)
X_val=np.c_[ X_val, np.ones(len(X_val))];

#1-(sum of squares of residual errors/sum of squares of error) 
def r2score(y,f):
  s1=np.sum(np.square(y-f));
  s2=np.sum(np.square(np.mean(y)-y))
  return (1-1.0*s1/s2)

"""# **MSE+lamda(L2)**"""

result=[];
#Lamda values for regularization
lamda=np.array([0.01,0.1,1,10,100,1000,10000,100000])
for l in lamda:
  result.append(Gradient_Descent (X, X_val, t, t_val, w, l, 1000, 1e-10, 1e-8, L2_Loss,L2_Gradient))
  #return w, train_loss_final, validation_loss_final, validation_NRMSE

ans1=np.array(result)
train_rmse=[];validation_rmse=[];
#Training and validation rmse calculations
for i in range(len(result)):
  train_rmse.append(np.sqrt(MSE_Loss(X,t,ans1[i][0])))
  validation_rmse.append(np.sqrt(MSE_Loss(X_val,t_val,ans1[i][0])))

"""# **First approximately plot the lamda vs rmse error then increase iterations to find the weights more accurately**"""

#Plotting different metrics
plt.plot(np.log10(1/lamda),train_rmse);plt.title("Training RMSE");plt.show()
plt.plot(np.log10(1/lamda),validation_rmse);plt.title("Validation RMSE");plt.show()
print("\nweights",ans1[np.argmin(ans1[:,3])][0])
print("\nvalidation rmse=",np.sqrt(MSE_Loss(X_val,t_val,ans1[np.argmin(ans1[:,3])][0])))
print("\nNRMSE loss=",ans1[np.argmin(ans1[:,3])][3])

"""### **Best lamda simulation**-After simulating for small number of iterations we get approximate lamda which gives us least validation rmse,then we increase the number of iterations to update weights more accurately.This done to reduce the time complexity of overall process.

"""

best1=Gradient_Descent (X, X_val, t, t_val, w, 10000, 100000, 1e-10, 1e-8, L2_Loss,L2_Gradient)

print("weights=",best1[0])
print("\nvalidation rmse=",best1[3]*np.std(t_val))
print("\nvalidation Nrmse=",best1[3])

"""# **MSE lamda(L1)**"""

result=[]
#Lamda values for regularization
lamda=np.array([0.0001,0.001,0.01,0.1,1,10,100,1000,10000,100000])
for l in lamda:
  result.append(Gradient_Descent (X, X_val, t, t_val, w, l, 1000, 1e-10, 1e-8, L1_Loss,L1_Gradient))
  #return w, train_loss_final, validation_loss_final, validation_NRMSE

ans1=np.array(result)
train_rmse=[];validation_rmse=[]
#Training and validation rmse calculations
for i in range(len(result)):
  train_rmse.append(np.sqrt(MSE_Loss(X,t,ans1[i][0])))
  validation_rmse.append(np.sqrt(MSE_Loss(X_val,t_val,ans1[i][0])))

"""# **First approximately plot the lamda vs rmse error then increase iterations to find the weights more accurately**"""

#Plotting different metrics
plt.plot(np.log10(1/lamda),train_rmse);plt.title("Training RMSE");plt.show()
plt.plot(np.log10(1/lamda),validation_rmse);plt.title("Validation RMSE");plt.show()
print("\nweights",ans1[np.argmin(ans1[:,3])][0])
print("\nvalidation rmse=",np.sqrt(MSE_Loss(X_val,t_val,ans1[np.argmin(ans1[:,3])][0])))
print("\nNRMSE loss=",ans1[np.argmin(ans1[:,3])][3])

"""### **Best lamda simulation**-After simulating for small number of iterations we get approximate lamda which gives us least validation rmse,then we increase the number of iterations to update weights more accurately.This done to reduce the time complexity of overall process.

"""

best2=Gradient_Descent (X, X_val, t, t_val, w, 1000, 100000, 1e-10, 1e-8, L1_Loss,L1_Gradient)

print("weights=",best2[0])
print("\nvalidation rmse=",best2[3]*np.std(t_val))
print("\nvalidation Nrmse=",best2[3])

"""# **Pseudo Inverse**"""

def Pseudo_Inverse_part2 (X, t, lamda): # Output should be weight vector
    r=len(X)
    X_new=np.c_[ X, np.ones(r)];
    I=np.identity(len(X_new[0]))
    X_t=np.transpose(X_new);
    w=np.dot(np.dot(np.linalg.inv((np.dot(X_t,X_new)+lamda*I)),X_t),t);
    return w
    raise NotImplementedError()

result=[]
lamda=np.array([i for i in range(1,50000)])

for l in lamda:
  w=Pseudo_Inverse_part2(X,t,l)
  trmse=np.sqrt(MSE_Loss(X,t,w))
  vrmse=np.sqrt(MSE_Loss(X_val,t_val,w))
  nrmse=NRMSE_Metric(X_val,t_val,w)
  result.append([w,trmse,vrmse,nrmse])

ans3=np.array(result)

plt.plot(1/lamda,ans3[:,1]);plt.title("Traing-Loss RMSE");plt.show()
plt.plot(1/lamda,ans3[:,2]);plt.title("Validation-Loss RMSE");plt.show()

print("weights",ans3[np.argmin(ans3[:,3])][0])
print("\nvalidation rmse=",ans3[np.argmin(ans3[:,3])][2])
print("\nNRMSE loss=",ans3[np.argmin(ans3[:,3])][3])

best3=ans3[np.argmin(ans3[:,3])]
print(Prediction(X_val,best3[0]),w,np.argmin(ans3[:,3]))

"""# **Predicted vs Actual value plot & R2 score for best model**"""

#Plotting for best model ie PseudoInverse
plt.scatter(Prediction(X_val,best3[0]),t_val);plt.title("Predicted vs Actual")
plt.show()
print("R2-score=",r2score(t_val,Prediction(X_val,best3[0])))

"""# **Test Prediction** """

#Preparing Test dataframe & saving predicted output
url="https://www.ee.iitb.ac.in/~asethi/Dump/TempTest.csv "
test_df=pd.read_csv(url)
#X_test=Normalize(np.asarray(test_df));display(test_df)
X_test=np.asarray(test_df);display(test_df)
X_test=np.c_[ X_test, np.ones(len(X_test))]
w=best3[0];print(w)

T=Prediction(X_test,w);print(T,np.mean(T),np.std(T),best3[0])
DF = pd.DataFrame(T)
#DF.to_csv("19D070048_1.csv")

"""# **Observation & Conclusion**
i)MSE+L2_Loss /MSE+L1_Loss(With Normalization)-


> 



1.   As 1/λ increases Training Rmse  decreases.
2.   The approximate λ=1000 for which validation rmse is minimum.
3.   The graph of validation rmse is convex curve.





ii)Pseudo Inverse(With Normalization)


> Indented block

1.   As 1/λ increases Training Rmse  decreases.
2.   The approximate λ=16812 for which validation rmse is minimum.
3.   As λ increases(beyond 50000) the weights start becoming ≈0 ,The targets predicted ≈0 hence validation rmse saturates at ≈30.To find the λ that reduces error we iterate over the convex part of the curve ie (λ =10000,50000) & we get rmse=4.5.

Test Prediction-
(With Normalization)

1.   MSE+L2: λ=1000,max_iteration=100000,mean of test target predicted=26.06,std_deviation=0.009,training validation rmse= 21.973113561377335,training validation Nrmse= 5.749226493586081


2.  MSE+L1:λ=1000,max_iteration=100000,mean of test target predicted=26.67,std_deviation=0.024,training validation rmse= 5.371868964734046,training NRMSE loss= 1.4055400608499795

3. Pseudo Inverse:λ=16812,mean of test target=10.99,std_deviation=1.34.training validation rmse= 4.508485693244155,training NRMSE loss= 1.179637347304018


With Normalization the Pseudoinverse method fails badly as mean,std_deviation of test is different from training .Similarly for L2,L1 regularization the variance is very less hence the output is skewed.

Test Prediction-
(Without Normalization)
1.   MSE+L2: λ=10000,max_iteration=100000,mean of test target predicted=28.70,std_deviation=2.264,training validation rmse= 5.53,training  Nrmse loss= 1.44


2.  MSE+L1:λ=10000,max_iteration=100000,mean of test target predicted=29.52,std_deviation=2.33,training validation rmse= 5.07,training NRMSE loss= 1.32

3. Pseudo Inverse:λ=1417,mean of test target=,std_deviation=.training validation rmse= 1.414,training NRMSE loss= 0.37

**PseudoInverse is the best in terms of nrmse loss.**

#**... Part 2 ends.**

1. Write the name or roll no.s of friends from outside your group with whom you discussed the assignment here (no penalty for mere discussion without copying code): 
2. Write the links of sources on the internet referred here (no penalty for mere consultation without copying code):
"""